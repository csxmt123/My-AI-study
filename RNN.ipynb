{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhouwt612/My-AI-study/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz8LV71ZJhlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# set random seed for comparing the two result calculations\n",
        "tf.set_random_seed(1)\n",
        "\n",
        "# this is data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "# hyperparameters\n",
        "lr = 0.001\n",
        "training_iters = 100000\n",
        "batch_size = 128\n",
        "\n",
        "n_inputs = 28   # MNIST data input (img shape: 28*28)\n",
        "n_steps = 28    # time steps\n",
        "n_hidden_units = 128   # neurons in hidden layer\n",
        "n_classes = 10      # MNIST classes (0-9 digits)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Define weights\n",
        "weights = {\n",
        "    # (28, 128)\n",
        "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
        "    # (128, 10)\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    # (128, )\n",
        "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
        "    # (10, )\n",
        "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
        "}\n",
        "\n",
        "\n",
        "def RNN(X, weights, biases):\n",
        "    # hidden layer for input to cell\n",
        "    ########################################\n",
        "\n",
        "    # transpose the inputs shape from\n",
        "    # X ==> (128 batch * 28 steps, 28 inputs)\n",
        "    X = tf.reshape(X, [-1, n_inputs])\n",
        "\n",
        "    # into hidden\n",
        "    # X_in = (128 batch * 28 steps, 128 hidden)\n",
        "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
        "    # X_in ==> (128 batch, 28 steps, 128 hidden)\n",
        "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
        "\n",
        "    # cell\n",
        "    ##########################################\n",
        "\n",
        "    # basic LSTM Cell.\n",
        "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "        cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
        "    else:\n",
        "        cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units)\n",
        "    # lstm cell is divided into two parts (c_state, h_state)\n",
        "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "    # You have 2 options for following step.\n",
        "    # 1: tf.nn.rnn(cell, inputs);\n",
        "    # 2: tf.nn.dynamic_rnn(cell, inputs).\n",
        "    # If use option 1, you have to modified the shape of X_in, go and check out this:\n",
        "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
        "    # In here, we go for option 2.\n",
        "    # dynamic_rnn receive Tensor (batch, steps, inputs) or (steps, batch, inputs) as X_in.\n",
        "    # Make sure the time_major is changed accordingly.\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)\n",
        "\n",
        "    # hidden layer for output as the final results\n",
        "    #############################################\n",
        "    # results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
        "\n",
        "    # # or\n",
        "    # unpack to list [(batch, outputs)..] * steps\n",
        "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "        outputs = tf.unpack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
        "    else:\n",
        "        outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
        "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']    # shape = (128, 10)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "pred = RNN(x, weights, biases)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # tf.initialize_all_variables() no long valid from\n",
        "    # 2017-03-02 if using tensorflow >= 0.12\n",
        "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "        init = tf.initialize_all_variables()\n",
        "    else:\n",
        "        init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    step = 0\n",
        "    while step * batch_size < training_iters:\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
        "        sess.run([train_op], feed_dict={\n",
        "            x: batch_xs,\n",
        "            y: batch_ys,\n",
        "        })\n",
        "        if step % 20 == 0:\n",
        "            print(sess.run(accuracy, feed_dict={\n",
        "            x: batch_xs,\n",
        "            y: batch_ys,\n",
        "            }))\n",
        "        step += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9KXcSYlJkaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "# hyperparameters\n",
        "lr = 0.001\n",
        "training_iters = 100000\n",
        "batch_size = 128\n",
        "\n",
        "n_inputs = 28\n",
        "n_steps = 28\n",
        "n_hidden_unis = 128\n",
        "n_classes = 10\n",
        "\n",
        "# tf.Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Define weights\n",
        "weights = {\n",
        "    # (28, 128)\n",
        "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_unis])),\n",
        "    # (128, 10)\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_unis, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    # (128, )\n",
        "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_unis, ])),\n",
        "    # (10, )\n",
        "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
        "}\n",
        "\n",
        "\n",
        "def RNN(X, weights, biases):\n",
        "    # hidden layer for input to cell\n",
        "    # X(128 batch, 28 steps, 28 inputs)\n",
        "    # ==>(128*28, 28, inputs\n",
        "    X = tf.reshape(X, [-1, n_inputs])\n",
        "    # ==>(128 batch*28 steps, 128 hidden)\n",
        "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
        "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_unis])\n",
        "\n",
        "    # cell\n",
        "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_unis, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
        "    # outputs\n",
        "    results = tf.matmul(states[1], weights['out']) + biases['out']\n",
        "    return results\n",
        "\n",
        "\n",
        "pre = RNN(x, weights, biases)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pre, labels=y))\n",
        "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
        "\n",
        "correct_pre = tf.equal(tf.argmax(pre, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    step = 0\n",
        "    while step*batch_size < training_iters:\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
        "        sess.run([train_op], feed_dict={\n",
        "            x: batch_xs,\n",
        "            y: batch_ys\n",
        "        })\n",
        "        if step % 20 == 0:\n",
        "            print(sess.run(accuracy, feed_dict={\n",
        "                x: batch_xs,\n",
        "                y: batch_ys\n",
        "            }))\n",
        "            step += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-yy7U_IJmBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
